import { Ollama } from "ollama";

// =====================
// Configuration Ollama
// =====================

// Instance du client Ollama (connexion locale par d√©faut)
const ollama = new Ollama();

/**
 * Configuration du timeout pour les requ√™tes Ollama
 *
 * IMPORTANT : Les mod√®les locaux peuvent √™tre lents selon votre mat√©riel
 * - Mod√®les l√©gers (< 7B) : ~5-15 secondes
 * - Mod√®les moyens (7B-13B) : ~15-45 secondes
 * - Mod√®les lourds (> 13B) : ~45-120 secondes
 *
 * Valeurs recommand√©es :
 * - 0 (d√©faut) : Pas de timeout (recommand√© pour d√©veloppement local)
 * - 120000 : 2 minutes (pour production avec mod√®les moyens)
 * - 180000 : 3 minutes (pour mod√®les lourds)
 *
 * Configuration via variable d'environnement : OLLAMA_TIMEOUT=120000
 */
const OLLAMA_TIMEOUT = process.env.OLLAMA_TIMEOUT
	? parseInt(process.env.OLLAMA_TIMEOUT)
	: 0;

// Validation du timeout
if (OLLAMA_TIMEOUT < 0) {
	console.warn(
		"‚ö†Ô∏è OLLAMA_TIMEOUT doit √™tre >= 0, utilisation de la valeur par d√©faut (0)"
	);
}

console.log(
	`‚öôÔ∏è Configuration Ollama timeout: ${
		OLLAMA_TIMEOUT === 0 ? "D√©sactiv√©" : `${OLLAMA_TIMEOUT / 1000}s`
	}`
);

// Catalogue des mod√®les disponibles (du plus l√©ger au plus lourd)
const MODELS = {
	// Mod√®le l√©ger g√©n√©raliste (par d√©faut)
	lightweight: "gpt-oss",
	// Mod√®le pour la programmation
	code: "hir0rameel/qwen-claude",
	// Mod√®le de secours si les autres √©chouent
	fallback: "gpt-oss",
};

// =====================
// Utilitaires
// =====================

/**
 * Ex√©cute un appel Ollama avec timeout optionnel
 * @param {Function} asyncFn - Fonction asynchrone √† ex√©cuter
 * @param {number} timeout - Timeout en ms (0 = pas de timeout)
 * @returns {Promise} - R√©sultat ou erreur timeout
 */
async function withTimeout(asyncFn, timeout = OLLAMA_TIMEOUT) {
	// Si timeout est 0 ou non d√©fini, pas de timeout
	if (!timeout || timeout === 0) {
		return asyncFn();
	}

	return Promise.race([
		asyncFn(),
		new Promise((_, reject) =>
			setTimeout(
				() => reject(new Error(`Ollama timeout after ${timeout / 1000}s`)),
				timeout
			)
		),
	]);
}

// =====================
// S√©lection intelligente du mod√®le
// =====================

/**
 * Choisit le mod√®le appropri√© selon le contenu de la note
 * @param {Object} note - La note √† analyser
 * @returns {string} - Le nom du mod√®le √† utiliser
 */
export function pickModel(note) {
	// Si tags IA contiennent "hir0rameel/qwen-claude" ‚Üí mod√®le code
	if (note.aiTags && note.aiTags.includes("hir0rameel/qwen-claude")) {
		return MODELS.code;
	}

	// Si la description contient des mots-cl√©s de programmation
	const codeKeywords = [
		"function",
		"variable",
		"class",
		"method",
		"code",
		"programming",
		"javascript",
		"python",
		"java",
		"const",
		"let",
		"var",
		"return",
		"import",
		"export",
	];

	const content = `${note.title || ""} ${note.description || ""}`.toLowerCase();
	const hasCodeKeywords = codeKeywords.some((keyword) =>
		content.includes(keyword.toLowerCase())
	);

	if (hasCodeKeywords) {
		return MODELS.code;
	}

	// Par d√©faut ‚Üí mod√®le l√©ger
	return MODELS.lightweight;
}

// =====================
// G√©n√©ration de question
// =====================

/**
 * G√©n√®re une question √† partir d'une note via Ollama
 * @param {Object} note - La note source
 * @returns {Promise<string>} - La question g√©n√©r√©e
 */
export async function generateQuestion(note) {
	const model = pickModel(note);
	const titlePart = note.title ? `Titre : ${note.title}\n` : "";

	const prompt = `Tu es un examinateur p√©dagogique. G√©n√®re UNE question courte et pr√©cise pour tester la compr√©hension de l'utilisateur (en pleine phase d'apprentissage) en fonction de sa note.

	${titlePart} Contenu : ${note.description}

	R√©ponds UNIQUEMENT avec la question, sans introduction ni explication.`;

	try {
		console.log(`ü§ñ G√©n√©ration de question avec le mod√®le: ${model}`);
		const startTime = Date.now();

		const response = await withTimeout(
			() =>
				ollama.generate({
					model,
					prompt,
					stream: false,
				}),
			OLLAMA_TIMEOUT
		);

		const duration = ((Date.now() - startTime) / 1000).toFixed(2);
		console.log(`‚úÖ Question g√©n√©r√©e avec succ√®s en ${duration}s`);
		return response.response.trim();
	} catch (error) {
		console.error(`‚ùå Erreur g√©n√©ration question (${model}):`, error.message);

		// Si erreur de timeout ou mod√®le introuvable, tenter le fallback
		if (model !== MODELS.fallback && !error.message.includes("timeout")) {
			console.log(`üîÑ Tentative avec le mod√®le fallback: ${MODELS.fallback}`);
			try {
				const fallbackResponse = await withTimeout(
					() =>
						ollama.generate({
							model: MODELS.fallback,
							prompt,
							stream: false,
						}),
					OLLAMA_TIMEOUT
				);
				console.log(`‚úÖ Question g√©n√©r√©e avec le mod√®le fallback`);
				return fallbackResponse.response.trim();
			} catch (fallbackError) {
				console.error(
					`‚ùå Erreur avec le mod√®le fallback:`,
					fallbackError.message
				);
			}
		}

		// Si tout √©choue, retourner une question simple
		console.log(`‚ö†Ô∏è Utilisation de la question par d√©faut`);
		return buildPrompt(note);
	}
}

export function buildPrompt(note) {
	const titlePart = note.title ? `Contexte / titre : ${note.title}\n\n` : "";
	return `Tu es un examinateur. Utilise la description suivante pour cr√©er une question qui teste la compr√©hension ou la m√©morisation. R√©ponds uniquement avec la question, puis attends la r√©ponse utilisateur.\n\n${titlePart}Description : ${note.description}\n\n.`;
}

// =====================
// √âvaluation de r√©ponse
// =====================

/**
 * √âvalue la r√©ponse de l'utilisateur via Ollama
 * @param {string} question - La question pos√©e
 * @param {string} userAnswer - La r√©ponse de l'utilisateur
 * @param {string} correctContext - Le contexte correct (description de la note)
 * @returns {Promise<Object>} - { isCorrect: boolean, feedback: string }
 */
export async function evaluateAnswer(question, userAnswer, correctContext) {
	// Utiliser le mod√®le l√©ger pour l'√©valuation (t√¢che simple)
	const model = MODELS.lightweight;

	const prompt = `√âvalue cette r√©ponse d'√©tudiant.

	Question : ${question}
	Contenu attendu : ${correctContext}
	R√©ponse de l'√©tudiant : ${userAnswer}

	R√©ponds en 2 lignes maximum :
	1. Premi√®re ligne : CORRECT ou INCORRECT
	2. Explication courte (1 phrase)`;

	try {
		console.log(`ü§ñ √âvaluation de la r√©ponse avec le mod√®le: ${model}`);
		const startTime = Date.now();

		const response = await withTimeout(
			() =>
				ollama.generate({
					model,
					prompt,
					stream: false,
				}),
			OLLAMA_TIMEOUT
		);

		const duration = ((Date.now() - startTime) / 1000).toFixed(2);
		const isCorrect = response.response.toLowerCase().includes("correct");
		console.log(
			`‚úÖ √âvaluation termin√©e en ${duration}s: ${
				isCorrect ? "CORRECT" : "INCORRECT"
			}`
		);

		return {
			isCorrect,
			feedback: response.response.trim(),
		};
	} catch (error) {
		console.error(`‚ùå Erreur √©valuation r√©ponse:`, error.message);

		// Fallback : √©valuation basique
		return {
			isCorrect: userAnswer.length > 10,
			feedback:
				"√âvaluation automatique temporairement indisponible. R√©ponse enregistr√©e.",
		};
	}
}

// =====================
// G√©n√©ration d'indice
// =====================

/**
 * G√©n√®re un indice pour aider l'utilisateur
 * @param {Object} note - La note source
 * @returns {Promise<string>} - L'indice g√©n√©r√©
 */
export async function generateHint(note) {
	const model = MODELS.lightweight;

	const prompt = `Donne UN indice court (1 phrase) pour aider √† r√©pondre √† une question sur ce sujet :

${note.description}

Indice :`;

	try {
		console.log(`ü§ñ G√©n√©ration d'indice avec le mod√®le: ${model}`);
		const startTime = Date.now();

		const response = await withTimeout(
			() =>
				ollama.generate({
					model,
					prompt,
					stream: false,
				}),
			OLLAMA_TIMEOUT
		);

		const duration = ((Date.now() - startTime) / 1000).toFixed(2);
		console.log(`‚úÖ Indice g√©n√©r√© avec succ√®s en ${duration}s`);
		return response.response.trim();
	} catch (error) {
		console.error(`‚ùå Erreur g√©n√©ration indice:`, error.message);
		return "Relisez attentivement le contexte de la note.";
	}
}

// =====================
// Fonctions legacy (pour compatibilit√©)
// =====================

export function pickIA(aiArray) {
	if (!Array.isArray(aiArray) || aiArray.length === 0) return null;
	return aiArray[0];
}
