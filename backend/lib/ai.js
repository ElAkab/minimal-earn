import { Ollama } from "ollama";

// =====================
// Configuration Ollama
// =====================

// Instance du client Ollama (connexion locale par d√©faut)
const ollama = new Ollama();

/**
 * Configuration du timeout pour les requ√™tes Ollama
 *
 * IMPORTANT : Les mod√®les locaux peuvent √™tre lents selon votre mat√©riel
 * - Mod√®les l√©gers (< 7B) : ~5-15 secondes
 * - Mod√®les moyens (7B-13B) : ~15-45 secondes
 * - Mod√®les lourds (> 13B) : ~45-120 secondes
 *
 * Valeurs recommand√©es :
 * - 0 (d√©faut) : Pas de timeout (recommand√© pour d√©veloppement local)
 * - 120000 : 2 minutes (pour production avec mod√®les moyens)
 * - 180000 : 3 minutes (pour mod√®les lourds)
 *
 * Configuration via variable d'environnement : OLLAMA_TIMEOUT=120000
 */
const OLLAMA_TIMEOUT = process.env.OLLAMA_TIMEOUT
	? parseInt(process.env.OLLAMA_TIMEOUT)
	: 0;

// Validation du timeout
if (OLLAMA_TIMEOUT < 0) {
	console.warn(
		"‚ö†Ô∏è OLLAMA_TIMEOUT doit √™tre >= 0, utilisation de la valeur par d√©faut (0)"
	);
}

console.log(
	`‚öôÔ∏è Configuration Ollama timeout: ${
		OLLAMA_TIMEOUT === 0 ? "D√©sactiv√©" : `${OLLAMA_TIMEOUT / 1000}s`
	}`
);

// Configuration des mod√®les IA par type de t√¢che
const AI_MODELS = {
	generation: {
		lightweight: "gpt-oss", // G√©n√©ration pour notes g√©n√©rales
		code: "hir0rameel/qwen-claude", // G√©n√©ration pour programmation
		fallback: "gpt-oss",
	},
	evaluation: {
		default: "gpt-oss", // √âvaluation rapide (toujours l√©ger)
		fallback: "gpt-oss",
	},
	hint: {
		default: "gpt-oss", // Indices rapides (toujours l√©ger)
		fallback: "gpt-oss",
	},
};

// Ancienne configuration (deprecated, conserv√© pour compatibilit√©)
const MODELS = {
	lightweight: "gpt-oss",
	code: "hir0rameel/qwen-claude",
	fallback: "gpt-oss",
};

// =====================
// Utilitaires
// =====================

/**
 * Ex√©cute un appel Ollama avec timeout optionnel
 * @param {Function} asyncFn - Fonction asynchrone √† ex√©cuter
 * @param {number} timeout - Timeout en ms (0 = pas de timeout)
 * @returns {Promise} - R√©sultat ou erreur timeout
 */
async function withTimeout(asyncFn, timeout = OLLAMA_TIMEOUT) {
	// Si timeout est 0 ou non d√©fini, pas de timeout
	if (!timeout || timeout === 0) {
		return asyncFn();
	}

	return Promise.race([
		asyncFn(),
		new Promise((_, reject) =>
			setTimeout(
				() => reject(new Error(`Ollama timeout after ${timeout / 1000}s`)),
				timeout
			)
		),
	]);
}

/**
 * Logger centralis√© pour tracer les d√©cisions IA
 * @param {string} task - Type de t√¢che
 * @param {string} model - Mod√®le s√©lectionn√©
 * @param {Object} metadata - M√©tadonn√©es additionnelles
 */
function logAIDecision(task, model, metadata = {}) {
	const timestamp = new Date().toISOString();
	const logEntry = {
		timestamp,
		task,
		model,
		...metadata,
	};

	console.log(`üìä [AI_DECISION] ${JSON.stringify(logEntry)}`);
}

// =====================
// S√©lection intelligente du mod√®le
// =====================

/**
 * S√©lectionne le mod√®le appropri√© selon la note ET la t√¢che
 * @param {Object} note - La note √† analyser
 * @param {'generation'|'evaluation'|'hint'} task - Type de t√¢che IA
 * @returns {string} - Nom du mod√®le √† utiliser
 */
export function pickModelForTask(note, task) {
	// Validation du param√®tre task
	const validTasks = ["generation", "evaluation", "hint"];
	if (!validTasks.includes(task)) {
		console.warn(
			`‚ö†Ô∏è T√¢che invalide: ${task}, utilisation de 'generation' par d√©faut`
		);
		task = "generation";
	}

	const taskModels = AI_MODELS[task];

	// Pour evaluation et hint : toujours utiliser le mod√®le par d√©faut (l√©ger)
	if (task === "evaluation" || task === "hint") {
		console.log(
			`üéØ T√¢che ${task} ‚Üí Mod√®le l√©ger par d√©faut: ${taskModels.default}`
		);
		logAIDecision(task, taskModels.default, {
			noteId: note.id,
			contentType: "n/a",
		});
		return taskModels.default;
	}

	// Pour generation : s√©lection dynamique selon le contenu
	// V√©rifier si tag IA explicite pour code
	if (note.aiTags && note.aiTags.includes("hir0rameel/qwen-claude")) {
		console.log(
			`üéØ T√¢che ${task} ‚Üí Tag IA d√©tect√© ‚Üí Mod√®le code: ${taskModels.code}`
		);
		logAIDecision(task, taskModels.code, {
			noteId: note.id,
			hasAiTags: true,
			contentType: "code",
		});
		return taskModels.code;
	}

	// D√©tecter mots-cl√©s de programmation
	const programmingKeywords = [
		"javascript",
		"code",
		"function",
		"variable",
		"class",
		"programming",
		"d√©veloppement",
		"algorithme",
	];

	const content = `${note.title || ""} ${note.description || ""}`.toLowerCase();
	const isProgramming = programmingKeywords.some((keyword) =>
		content.includes(keyword.toLowerCase())
	);

	if (isProgramming) {
		console.log(
			`üéØ T√¢che ${task} ‚Üí Contenu programmation d√©tect√© ‚Üí Mod√®le code: ${taskModels.code}`
		);
		logAIDecision(task, taskModels.code, {
			noteId: note.id,
			hasAiTags: !!note.aiTags,
			contentType: "code",
		});
		return taskModels.code;
	}

	// Par d√©faut : mod√®le l√©ger
	console.log(
		`üéØ T√¢che ${task} ‚Üí Contenu g√©n√©ral ‚Üí Mod√®le l√©ger: ${taskModels.lightweight}`
	);
	logAIDecision(task, taskModels.lightweight, {
		noteId: note.id,
		hasAiTags: !!note.aiTags,
		contentType: "general",
	});
	return taskModels.lightweight;
}

/**
 * @deprecated Utiliser pickModelForTask() √† la place
 * Conserv√© pour compatibilit√© avec les tests existants
 * Cette fonction sera supprim√©e dans une version future
 *
 * Choisit le mod√®le appropri√© selon le contenu de la note
 * @param {Object} note - La note √† analyser
 * @returns {string} - Le nom du mod√®le √† utiliser
 */
export function pickModel(note) {
	console.warn(
		"‚ö†Ô∏è pickModel() est deprecated, utiliser pickModelForTask() √† la place"
	);
	return pickModelForTask(note, "generation");
}

// =====================
// Fonctions de test (d√©veloppement)
// =====================

/**
 * Teste un mod√®le Ollama avec une question personnalis√©e
 * Utile pour v√©rifier que les mod√®les fonctionnent correctement
 *
 * @param {string} modelName - Nom du mod√®le √† tester (ex: "gpt-oss", "hir0rameel/qwen-claude")
 * @param {string} question - Question √† poser au mod√®le
 * @returns {Promise<Object>} - { success: boolean, response: string, duration: number, error?: string }
 *
 * @example
 * // Tester le mod√®le l√©ger
 * await testModel("gpt-oss", "Qu'est-ce que JavaScript ?");
 *
 * // Tester le mod√®le code
 * await testModel("hir0rameel/qwen-claude", "Explique le concept de closure en JavaScript");
 */
export async function testModel(modelName, question) {
	console.log(`\nüß™ Test du mod√®le: ${modelName}`);
	console.log(`üìù Question: "${question}"`);
	console.log(
		`‚è±Ô∏è  Timeout configur√©: ${
			OLLAMA_TIMEOUT === 0 ? "D√©sactiv√©" : `${OLLAMA_TIMEOUT / 1000}s`
		}`
	);
	console.log(`‚è≥ Envoi de la requ√™te...\n`);

	const startTime = Date.now();

	try {
		const response = await withTimeout(
			() =>
				ollama.generate({
					model: modelName,
					prompt: question,
					stream: false,
				}),
			OLLAMA_TIMEOUT
		);

		const duration = ((Date.now() - startTime) / 1000).toFixed(2);

		console.log(`‚úÖ R√©ponse re√ßue en ${duration}s`);
		console.log(`\nüìÑ R√©ponse du mod√®le:\n${"=".repeat(50)}`);
		console.log(response.response.trim());
		console.log(`${"=".repeat(50)}\n`);

		return {
			success: true,
			response: response.response.trim(),
			duration: parseFloat(duration),
			model: modelName,
		};
	} catch (error) {
		const duration = ((Date.now() - startTime) / 1000).toFixed(2);

		console.error(`‚ùå Erreur apr√®s ${duration}s`);
		console.error(`   Type: ${error.message}`);
		console.error(`   Mod√®le: ${modelName}\n`);

		return {
			success: false,
			response: null,
			duration: parseFloat(duration),
			model: modelName,
			error: error.message,
		};
	}
}

// =====================
// G√©n√©ration de question
// =====================

/**
 * G√©n√®re une question √† partir d'une note via Ollama
 * @param {Object} note - La note source
 * @returns {Promise<{question: string, model: string}>} - Objet contenant la question g√©n√©r√©e et le mod√®le utilis√©
 */
export async function generateQuestion(note) {
	const model = pickModelForTask(note, "generation");
	const titlePart = note.title ? `Titre : ${note.title}\n` : "";

	const prompt = `Tu es un examinateur p√©dagogique. G√©n√®re UNE question courte et pr√©cise pour tester la compr√©hension de l'utilisateur (en pleine phase d'apprentissage) en fonction de sa note.

	${titlePart}Contenu : ${note.description}

	Tu peux utiliser Markdown pour formater ta question (gras **texte**, italique *texte*, code \`code\`, listes, etc.).
	R√©ponds UNIQUEMENT avec la question, sans introduction ni explication.`;

	try {
		console.log(`ü§ñ [GENERATION] Note ${note.id} ‚Üí Mod√®le: ${model}`);
		const startTime = Date.now();

		const response = await withTimeout(
			() =>
				ollama.generate({
					model,
					prompt,
					stream: false,
				}),
			OLLAMA_TIMEOUT
		);

		const duration = ((Date.now() - startTime) / 1000).toFixed(2);
		console.log(
			`‚úÖ [GENERATION] Question g√©n√©r√©e (${duration}s) avec ${model}`
		);
		return {
			question: response.response.trim(),
			model: model,
		};
	} catch (error) {
		console.error(`‚ùå [GENERATION] Erreur avec ${model}:`, error.message);

		// Si erreur de timeout ou mod√®le introuvable, tenter le fallback
		if (
			model !== AI_MODELS.generation.fallback &&
			!error.message.includes("timeout")
		) {
			console.log(
				`üîÑ [GENERATION] Tentative fallback: ${AI_MODELS.generation.fallback}`
			);
			try {
				const fallbackResponse = await withTimeout(
					() =>
						ollama.generate({
							model: AI_MODELS.generation.fallback,
							prompt,
							stream: false,
						}),
					OLLAMA_TIMEOUT
				);
				console.log(`‚úÖ Question g√©n√©r√©e avec le mod√®le fallback`);
				return {
					question: fallbackResponse.response.trim(),
					model: AI_MODELS.generation.fallback,
				};
			} catch (fallbackError) {
				console.error(
					`‚ùå Erreur avec le mod√®le fallback:`,
					fallbackError.message
				);
			}
		}

		// Si tout √©choue, retourner une question simple
		console.log(`‚ö†Ô∏è Utilisation de la question par d√©faut`);
		return {
			question: buildPrompt(note),
			model: AI_MODELS.generation.fallback,
		};
	}
}

export function buildPrompt(note) {
	const titlePart = note.title ? `Contexte / titre : ${note.title}\n\n` : "";
	return `Tu es un examinateur. Utilise la description suivante pour cr√©er une question qui teste la compr√©hension ou la m√©morisation. R√©ponds uniquement avec la question, puis attends la r√©ponse utilisateur.\n\n${titlePart}Description : ${note.description}\n\n.`;
}

// =====================
// √âvaluation de r√©ponse
// =====================

/**
 * √âvalue la r√©ponse de l'utilisateur via Ollama
 * @param {string} question - La question pos√©e
 * @param {string} userAnswer - La r√©ponse de l'utilisateur
 * @param {string} correctContext - Le contexte correct (description de la note)
 * @returns {Promise<Object>} - { isCorrect: boolean, feedback: string }
 */
export async function evaluateAnswer(question, userAnswer, correctContext) {
	// Cr√©er un objet note minimal pour pickModelForTask
	const noteContext = { title: "", description: correctContext };
	const model = pickModelForTask(noteContext, "evaluation");

	const prompt = `√âvalue cette r√©ponse d'√©tudiant.

	Question : ${question}
	Contenu attendu : ${correctContext}
	R√©ponse de l'√©tudiant : ${userAnswer}

	R√©ponds en 2 lignes maximum :
	1. Premi√®re ligne : CORRECT ou INCORRECT  
	2. Explication courte (tu peux utiliser Markdown : **gras**, *italique*, \`code\`)`;

	try {
		console.log(
			`ü§ñ [EVALUATION] Question: "${question.substring(
				0,
				50
			)}..." ‚Üí Mod√®le: ${model}`
		);
		const startTime = Date.now();

		const response = await withTimeout(
			() =>
				ollama.generate({
					model,
					prompt,
					stream: false,
				}),
			OLLAMA_TIMEOUT
		);

		const duration = ((Date.now() - startTime) / 1000).toFixed(2);
		const isCorrect = response.response.toLowerCase().includes("correct");
		console.log(
			`‚úÖ [EVALUATION] R√©sultat: ${
				isCorrect ? "CORRECT" : "INCORRECT"
			} (${duration}s) avec ${model}`
		);

		return {
			isCorrect,
			feedback: response.response.trim(),
		};
	} catch (error) {
		console.error(`‚ùå [EVALUATION] Erreur avec ${model}:`, error.message);

		// Fallback : √©valuation basique
		return {
			isCorrect: userAnswer.length > 10,
			feedback:
				"√âvaluation automatique temporairement indisponible. R√©ponse enregistr√©e.",
		};
	}
}

// =====================
// G√©n√©ration d'indice
// =====================

/**
 * G√©n√®re un indice pour aider l'utilisateur
 * @param {Object} note - La note source
 * @returns {Promise<string>} - L'indice g√©n√©r√©
 */
export async function generateHint(note) {
	const model = pickModelForTask(note, "hint");

	const prompt = `Donne UN indice court (1 phrase) pour aider √† r√©pondre √† une question sur ce sujet :

${note.description}

Tu peux utiliser Markdown pour le formatage (**gras**, *italique*, \`code\`).
Indice :`;

	try {
		console.log(`ü§ñ [HINT] Note ${note.id} ‚Üí Mod√®le: ${model}`);
		const startTime = Date.now();

		const response = await withTimeout(
			() =>
				ollama.generate({
					model,
					prompt,
					stream: false,
				}),
			OLLAMA_TIMEOUT
		);

		const duration = ((Date.now() - startTime) / 1000).toFixed(2);
		console.log(`‚úÖ [HINT] Indice g√©n√©r√© (${duration}s) avec ${model}`);
		return response.response.trim();
	} catch (error) {
		console.error(`‚ùå [HINT] Erreur avec ${model}:`, error.message);
		return "Relisez attentivement le contexte de la note.";
	}
}

// =====================
// Fonctions legacy (pour compatibilit√©)
// =====================

export function pickIA(aiArray) {
	if (!Array.isArray(aiArray) || aiArray.length === 0) return null;
	return aiArray[0];
}
