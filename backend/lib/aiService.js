import { Ollama } from "ollama";

/**
 * aiService.js
 *
 * Service centralis√© pour tous les appels IA.
 * Impl√©mente une strat√©gie hybride :
 * - IA externe rapide par d√©faut (OpenRouter, OpenAI, etc.)
 * - Ollama local en fallback ou sur demande
 *
 * Architecture :
 * - Tous les appels IA passent par ce module
 * - Configuration flexible via variables d'environnement
 * - Support de plusieurs providers
 * - Gestion des timeouts et erreurs
 * - M√©triques et logging
 */

// =====================
// CONFIGURATION
// =====================

/**
 * Configuration du provider IA par d√©faut
 * Valeurs possibles : "ollama", "openrouter", "openai", "anthropic"
 *
 * D√©faut : "ollama" (IA locale gratuite)
 * Pour production : "openrouter" (IA externe rapide, peu co√ªteuse)
 */
const AI_PROVIDER = process.env.AI_PROVIDER || "ollama";

/**
 * Configuration des API keys (pour providers externes)
 */
const API_KEYS = {
	openrouter: process.env.OPENROUTER_API_KEY,
	openai: process.env.OPENAI_API_KEY,
	anthropic: process.env.ANTHROPIC_API_KEY,
};

/**
 * Configuration des mod√®les par provider et t√¢che
 */
const MODELS_CONFIG = {
	ollama: {
		generation: "gpt-oss", // Mod√®le l√©ger pour g√©n√©ration
		evaluation: "gpt-oss", // M√™me mod√®le pour √©valuation
		hint: "gpt-oss", // M√™me mod√®le pour indices
		code: "hir0rameel/qwen-claude", // Mod√®le sp√©cialis√© pour code
	},
	openrouter: {
		generation: "google/gemini-flash-1.5-8b", // Rapide et peu cher
		evaluation: "google/gemini-flash-1.5-8b", // Idem
		hint: "google/gemini-flash-1.5-8b", // Idem
		code: "google/gemini-flash-1.5-8b", // Peut g√©rer le code aussi
	},
};

/**
 * Timeouts par provider (ms)
 */
const TIMEOUTS = {
	ollama: 30000, // 30s pour IA locale
	openrouter: 10000, // 10s pour IA externe
	openai: 10000,
	anthropic: 10000,
};

// Instance Ollama
const ollama = new Ollama();

// =====================
// S√âLECTION DU MOD√àLE
// =====================

/**
 * D√©termine le mod√®le √† utiliser selon le provider, la t√¢che et le contenu
 * @param {string} task - "generation", "evaluation", "hint"
 * @param {Object} note - Note √† analyser (optionnel)
 * @returns {string} - Nom du mod√®le
 */
export function selectModel(task, note = null) {
	const provider = AI_PROVIDER;
	const models = MODELS_CONFIG[provider];

	if (!models) {
		console.warn(`‚ö†Ô∏è Provider inconnu: ${provider}, utilisation d'ollama`);
		return MODELS_CONFIG.ollama[task] || MODELS_CONFIG.ollama.generation;
	}

	// Pour Ollama : d√©tecter si c'est du code
	if (provider === "ollama" && note) {
		const codeKeywords = [
			"function",
			"variable",
			"class",
			"method",
			"code",
			"javascript",
			"python",
			"const",
			"let",
			"var",
			"return",
		];

		const content = `${note.title || ""} ${
			note.description || ""
		}`.toLowerCase();
		const isCode = codeKeywords.some((keyword) => content.includes(keyword));

		if (isCode && task === "generation") {
			return models.code;
		}
	}

	return models[task] || models.generation;
}

// =====================
// APPELS IA PAR PROVIDER
// =====================

/**
 * G√©n√®re du texte via Ollama
 * @param {string} model - Nom du mod√®le
 * @param {string} prompt - Prompt
 * @returns {Promise<string>} - R√©ponse g√©n√©r√©e
 */
async function generateWithOllama(model, prompt) {
	const response = await ollama.generate({
		model,
		prompt,
		stream: false,
	});
	return response.response.trim();
}

/**
 * G√©n√®re du texte via OpenRouter
 * @param {string} model - Nom du mod√®le
 * @param {string} prompt - Prompt
 * @returns {Promise<string>} - R√©ponse g√©n√©r√©e
 */
async function generateWithOpenRouter(model, prompt) {
	const apiKey = API_KEYS.openrouter;
	if (!apiKey) {
		throw new Error("OPENROUTER_API_KEY non configur√©e");
	}

	const response = await fetch(
		"https://openrouter.ai/api/v1/chat/completions",
		{
			method: "POST",
			headers: {
				Authorization: `Bearer ${apiKey}`,
				"Content-Type": "application/json",
				"HTTP-Referer": "http://localhost:5173", // Optionnel
			},
			body: JSON.stringify({
				model,
				messages: [{ role: "user", content: prompt }],
			}),
		}
	);

	if (!response.ok) {
		const error = await response.text();
		throw new Error(`OpenRouter error: ${error}`);
	}

	const data = await response.json();
	return data.choices[0].message.content.trim();
}

/**
 * G√©n√®re du texte selon le provider configur√©
 * @param {string} model - Nom du mod√®le
 * @param {string} prompt - Prompt
 * @returns {Promise<string>} - R√©ponse g√©n√©r√©e
 */
async function generateText(model, prompt) {
	const provider = AI_PROVIDER;
	const timeout = TIMEOUTS[provider] || 30000;

	const generateFn = async () => {
		switch (provider) {
			case "ollama":
				return await generateWithOllama(model, prompt);
			case "openrouter":
				return await generateWithOpenRouter(model, prompt);
			default:
				throw new Error(`Provider non support√©: ${provider}`);
		}
	};

	// Appliquer timeout
	return Promise.race([
		generateFn(),
		new Promise((_, reject) =>
			setTimeout(() => reject(new Error(`Timeout apr√®s ${timeout}ms`)), timeout)
		),
	]);
}

// =====================
// FONCTIONS PUBLIQUES
// =====================

/**
 * G√©n√®re une question pour une note
 * @param {Object} note - Note source
 * @returns {Promise<Object>} - { question, model }
 */
export async function generateQuestion(note) {
	const model = selectModel("generation", note);
	const titlePart = note.title ? `Titre : ${note.title}\n` : "";

	const prompt = `Tu es un examinateur p√©dagogique. G√©n√®re UNE question courte et pr√©cise pour tester la compr√©hension de l'utilisateur en phase d'apprentissage.

${titlePart}Contenu : ${note.description}

R√©ponds UNIQUEMENT avec la question, sans introduction ni explication.`;

	try {
		console.log(
			`ü§ñ G√©n√©ration question (provider: ${AI_PROVIDER}, model: ${model})`
		);
		const startTime = Date.now();

		const question = await generateText(model, prompt);

		const duration = ((Date.now() - startTime) / 1000).toFixed(2);
		console.log(`‚úÖ Question g√©n√©r√©e en ${duration}s`);

		return { question, model };
	} catch (error) {
		console.error(`‚ùå Erreur g√©n√©ration question:`, error.message);

		// Fallback vers Ollama si provider externe √©choue
		if (AI_PROVIDER !== "ollama") {
			console.log(`üîÑ Tentative avec Ollama...`);
			try {
				const fallbackModel = MODELS_CONFIG.ollama.generation;
				const question = await generateWithOllama(fallbackModel, prompt);
				return { question, model: fallbackModel };
			} catch (fallbackError) {
				console.error(`‚ùå Fallback √©chou√©:`, fallbackError.message);
			}
		}

		// Dernier recours : question basique
		return {
			question: `Explique en d√©tail : ${note.description.substring(0, 100)}...`,
			model: "fallback",
		};
	}
}

/**
 * √âvalue la r√©ponse d'un utilisateur
 * @param {string} question - Question pos√©e
 * @param {string} userAnswer - R√©ponse de l'utilisateur
 * @param {string} correctContext - Contexte correct (description de la note)
 * @returns {Promise<Object>} - { isCorrect, feedback, confidence }
 */
export async function evaluateAnswer(question, userAnswer, correctContext) {
	const model = selectModel("evaluation");

	const prompt = `√âvalue cette r√©ponse d'√©tudiant.

Question : ${question}
Contenu attendu : ${correctContext}
R√©ponse de l'√©tudiant : ${userAnswer}

R√©ponds en 2 lignes maximum :
1. Premi√®re ligne : CORRECT ou INCORRECT
2. Explication courte (1 phrase)`;

	try {
		console.log(
			`ü§ñ √âvaluation r√©ponse (provider: ${AI_PROVIDER}, model: ${model})`
		);
		const startTime = Date.now();

		const response = await generateText(model, prompt);

		const duration = ((Date.now() - startTime) / 1000).toFixed(2);
		const isCorrect = response.toLowerCase().includes("correct");

		console.log(
			`‚úÖ √âvaluation termin√©e en ${duration}s: ${
				isCorrect ? "CORRECT" : "INCORRECT"
			}`
		);

		return {
			isCorrect,
			feedback: response,
			confidence: isCorrect ? 0.9 : 0.8, // Score de confiance (futur usage)
		};
	} catch (error) {
		console.error(`‚ùå Erreur √©valuation:`, error.message);

		// Fallback : √©valuation basique
		return {
			isCorrect: userAnswer.length > 10,
			feedback: "√âvaluation automatique indisponible. R√©ponse enregistr√©e.",
			confidence: 0.5,
		};
	}
}

/**
 * G√©n√®re un indice pour aider l'utilisateur
 * @param {Object} note - Note source
 * @returns {Promise<string>} - Indice g√©n√©r√©
 */
export async function generateHint(note) {
	const model = selectModel("hint", note);

	const prompt = `Donne UN indice court (1 phrase) pour aider √† r√©pondre √† une question sur ce sujet :

${note.description}

Indice :`;

	try {
		console.log(
			`ü§ñ G√©n√©ration indice (provider: ${AI_PROVIDER}, model: ${model})`
		);
		const startTime = Date.now();

		const hint = await generateText(model, prompt);

		const duration = ((Date.now() - startTime) / 1000).toFixed(2);
		console.log(`‚úÖ Indice g√©n√©r√© en ${duration}s`);

		return hint;
	} catch (error) {
		console.error(`‚ùå Erreur g√©n√©ration indice:`, error.message);
		return "Relisez attentivement le contexte de la note.";
	}
}

// =====================
// CONFIGURATION ET STATS
// =====================

/**
 * Retourne la configuration actuelle
 * @returns {Object} - Configuration
 */
export function getAIConfig() {
	return {
		provider: AI_PROVIDER,
		models: MODELS_CONFIG[AI_PROVIDER],
		timeout: TIMEOUTS[AI_PROVIDER],
		hasApiKey: !!API_KEYS[AI_PROVIDER],
	};
}

/**
 * Change le provider IA (pour page Param√®tres future)
 * @param {string} newProvider - Nouveau provider
 */
export function setAIProvider(newProvider) {
	if (!MODELS_CONFIG[newProvider]) {
		throw new Error(`Provider non support√©: ${newProvider}`);
	}
	process.env.AI_PROVIDER = newProvider;
	console.log(`‚öôÔ∏è Provider IA chang√©: ${newProvider}`);
}

// =====================
// FONCTIONS LEGACY (compatibilit√©)
// =====================

export function pickModel(note) {
	return selectModel("generation", note);
}

export function buildPrompt(note) {
	const titlePart = note.title ? `Contexte / titre : ${note.title}\n\n` : "";
	return `Tu es un examinateur. Utilise la description suivante pour cr√©er une question qui teste la compr√©hension ou la m√©morisation. R√©ponds uniquement avec la question, puis attends la r√©ponse utilisateur.\n\n${titlePart}Description : ${note.description}\n\n.`;
}

export function pickModelForTask(note, task) {
	return selectModel(task, note);
}
