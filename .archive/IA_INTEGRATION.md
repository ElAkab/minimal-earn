# ü§ñ Int√©gration IA - Documentation Technique

## üéØ Strat√©gie d'int√©gration

L'int√©gration IA suit une approche **modulaire, √©conome et strat√©gique** :

- ‚úÖ S√©paration stricte des r√¥les (g√©n√©ration vs √©valuation)
- ‚úÖ Priorit√© aux mod√®les l√©gers
- ‚úÖ S√©lection intelligente selon le contenu
- ‚úÖ Centralis√© dans `backend/lib/ai.js`
- ‚úÖ Aucun appel IA depuis le frontend

---

## üì¶ Mod√®les utilis√©s

### Configuration actuelle

```javascript
const MODELS = {
	lightweight: "gemma2:2b", // Mod√®le l√©ger g√©n√©raliste (par d√©faut)
	code: "qwen2.5-coder:3b", // Mod√®le pour la programmation
	fallback: "gemma2:2b", // Secours en cas d'erreur
};
```

### Crit√®res de s√©lection

La fonction `pickModel(note)` choisit automatiquement le mod√®le selon :

1. **Tags IA** : Si `claudeCode` dans `note.aiTags` ‚Üí mod√®le code
2. **Mots-cl√©s** : Si d√©tection de termes de programmation ‚Üí mod√®le code
3. **Par d√©faut** : Mod√®le l√©ger g√©n√©raliste

**Mots-cl√©s d√©tect√©s :**
`function`, `variable`, `class`, `method`, `code`, `programming`, `javascript`, `python`, `java`, `const`, `let`, `var`, `return`, `import`, `export`

---

## üîß Fonctions disponibles

### 1. **G√©n√©ration de question** ‚úÖ

**Fichier :** [ai.js](backend/lib/ai.js#L52-L75)

```javascript
await generateQuestion(note);
```

**Comportement :**

- S√©lectionne automatiquement le mod√®le appropri√©
- G√©n√®re une question courte et pr√©cise
- Prompt optimis√© pour √©viter les bavardages
- Fallback sur `buildPrompt()` en cas d'erreur

**API Route :**

```http
GET /api/generate-question/:id
```

**R√©ponse :**

```json
{
	"question": "Que signifie le terme 'Pointeur' en programmation ?",
	"model": "qwen2.5-coder:3b"
}
```

---

### 2. **√âvaluation de r√©ponse** ‚úÖ

**Fichier :** [ai.js](backend/lib/ai.js#L77-L114)

```javascript
await evaluateAnswer(question, userAnswer, correctContext);
```

**Comportement :**

- Utilise toujours le mod√®le l√©ger (t√¢che simple)
- Prompt strict : `CORRECT` ou `INCORRECT` + explication courte
- Maximum 2 lignes de r√©ponse
- Fallback sur √©valuation basique en cas d'erreur

**API Route :**

```http
POST /api/evaluate-answer
Content-Type: application/json

{
  "noteId": 1234,
  "question": "Question pos√©e",
  "userAnswer": "R√©ponse de l'utilisateur"
}
```

**R√©ponse :**

```json
{
	"isCorrect": true,
	"feedback": "CORRECT\nVotre r√©ponse d√©montre une bonne compr√©hension du concept."
}
```

---

### 3. **G√©n√©ration d'indice** ‚úÖ

**Fichier :** [ai.js](backend/lib/ai.js#L116-L136)

```javascript
await generateHint(note);
```

**Comportement :**

- Utilise le mod√®le l√©ger
- G√©n√®re un indice court (1 phrase)
- Fallback sur message g√©n√©rique en cas d'erreur

**API Route :**

```http
GET /api/hint/:id
```

**R√©ponse :**

```json
{
	"hint": "Pensez √† la mani√®re dont JavaScript g√®re l'absence de valeur de retour."
}
```

---

## üîí S√©curit√© & Performance

### Timeout

Tous les appels Ollama ont un **timeout de 30 secondes** :

```javascript
const OLLAMA_TIMEOUT = 30000; // 30 secondes
```

### Gestion d'erreur

Chaque fonction IA a un **fallback** en cas d'erreur :

| Fonction              | Fallback                                              |
| --------------------- | ----------------------------------------------------- |
| `generateQuestion`    | Retourne `buildPrompt(note)` (prompt template)       |
| `evaluateAnswer`      | √âvaluation basique (longueur > 10 caract√®res)         |
| `generateHint`        | Message g√©n√©rique "Relisez le contexte"               |

### Centralisation

**‚úÖ Tous les appels Ollama** sont centralis√©s dans `backend/lib/ai.js`

**‚ùå Aucun appel** depuis le frontend (s√©curit√© + performance)

---

## üìä Flux d'utilisation

### 1. **G√©n√©ration de question**

```
Note cr√©√©e (frontend)
     ‚Üì
POST /api/generate-note
     ‚Üì
Stockage dans notes.json
     ‚Üì
Scheduler calcule nextReviewAt
     ‚Üì
Frontend charge les notes dues
     ‚Üì
GET /api/generate-question/:id  ‚Üê Appel IA ici
     ‚Üì
Affichage dans review.html
```

### 2. **√âvaluation de r√©ponse**

```
Utilisateur r√©pond
     ‚Üì
POST /api/evaluate-answer  ‚Üê Appel IA ici
     ‚Üì
{ isCorrect, feedback }
     ‚Üì
POST /api/review-note (enregistre r√©sultat)
     ‚Üì
Scheduler adapte nextReviewAt
     ‚Üì
Affichage du feedback
```

### 3. **Demande d'indice**

```
Clic sur "Indice"
     ‚Üì
GET /api/hint/:id  ‚Üê Appel IA ici
     ‚Üì
Affichage en toast + contexte
```

---

## üß™ Comment tester

### Pr√©requis

1. **Ollama install√© et en cours d'ex√©cution** :

```bash
# V√©rifier que Ollama tourne
curl http://localhost:11434/api/version

# Si pas install√© :
# curl -fsSL https://ollama.com/install.sh | sh
```

2. **T√©l√©charger les mod√®les** :

```bash
# Mod√®le l√©ger g√©n√©raliste
ollama pull gemma2:2b

# Mod√®le code (optionnel)
ollama pull qwen2.5-coder:3b
```

### Test complet

1. **Cr√©er une note** :

```bash
curl -X POST http://localhost:5000/api/generate-note \
  -H "Content-Type: application/json" \
  -d '{
    "aiTags": ["claudeCode"],
    "title": "JavaScript",
    "description": "Les fonctions renvoient undefined par d√©faut",
    "intensity": "moderate"
  }'
```

2. **G√©n√©rer une question** :

```bash
curl http://localhost:5000/api/generate-question/1234
```

3. **√âvaluer une r√©ponse** :

```bash
curl -X POST http://localhost:5000/api/evaluate-answer \
  -H "Content-Type: application/json" \
  -d '{
    "noteId": 1234,
    "question": "Que renvoient les fonctions JavaScript par d√©faut ?",
    "userAnswer": "undefined"
  }'
```

4. **Demander un indice** :

```bash
curl http://localhost:5000/api/hint/1234
```

---

## üé® Interface utilisateur

### Modifications dans review.js

1. **Chargement de la question** ([review.js](src/review.js#L95-L112))

```javascript
// Ancien : r√©cup√®re un prompt template
const response = await fetch(`${API_URL}/prompt/${currentNote.id}`);

// Nouveau : g√©n√®re une vraie question IA
const response = await fetch(
	`${API_URL}/generate-question/${currentNote.id}`
);
```

2. **√âvaluation de la r√©ponse** ([review.js](src/review.js#L165-L202))

```javascript
// Ancien : simulation basique
const isCorrect = answer.length > 10;

// Nouveau : √©valuation IA compl√®te
const response = await fetch(`${API_URL}/evaluate-answer`, {
	method: "POST",
	body: JSON.stringify({ noteId, question, userAnswer }),
});
const evaluation = await response.json();
```

3. **G√©n√©ration d'indice** ([review.js](src/review.js#L215-L227))

```javascript
// Ancien : message statique
showToast("Indice : Relisez le contexte", "info");

// Nouveau : indice g√©n√©r√© par IA
const response = await fetch(`${API_URL}/hint/${currentNote.id}`);
const data = await response.json();
showToast(`üí° Indice : ${data.hint}`, "info");
```

---

## ‚ö° Optimisations futures

### Court terme

- [ ] Mise en cache des questions g√©n√©r√©es (√©viter reg√©n√©ration)
- [ ] Retry automatique en cas d'√©chec Ollama
- [ ] M√©triques de performance (temps de r√©ponse IA)

### Moyen terme

- [ ] Support de mod√®les multiples par cat√©gorie
- [ ] Ajustement automatique du mod√®le selon performance
- [ ] Syst√®me de prompt templating plus avanc√©

### Long terme

- [ ] Fine-tuning personnalis√© selon style d'apprentissage
- [ ] Analyse de progression pour ajuster difficult√©
- [ ] G√©n√©ration de statistiques d'efficacit√© IA

---

## üìù Logs et debugging

### Activer les logs d√©taill√©s

Les logs Ollama sont sobres par d√©faut. Pour debug :

```javascript
// Dans ai.js, ajouter apr√®s fetchOllama()
console.log("IA Request:", { model, promptLength: prompt.length });
console.log("IA Response:", { responseLength: response.length });
```

### Logs actuels

```javascript
console.error("Error generating question:", error);
console.error("Error evaluating answer:", error);
console.error("Error generating hint:", error);
```

---

## üîß Configuration personnalis√©e

### Changer les mod√®les

√âditer [ai.js](backend/lib/ai.js#L5-L12) :

```javascript
const MODELS = {
	lightweight: "gemma2:2b", // Remplacer par ton mod√®le pr√©f√©r√©
	code: "qwen2.5-coder:3b",
	fallback: "gemma2:2b",
};
```

### Ajuster le timeout

```javascript
const OLLAMA_TIMEOUT = 30000; // Augmenter si mod√®les lents
```

### Personnaliser les prompts

Les prompts sont dans les fonctions respectives :

- `generateQuestion()` - [ligne 58-66](backend/lib/ai.js#L58-L66)
- `evaluateAnswer()` - [ligne 89-97](backend/lib/ai.js#L89-L97)
- `generateHint()` - [ligne 124-129](backend/lib/ai.js#L124-L129)

---

## ‚úÖ Checklist de production

Avant de passer en "production" :

- [x] Tous les appels IA ont un timeout
- [x] Tous les appels IA ont un fallback
- [x] Gestion d'erreur robuste
- [x] Logs d'erreur (pas de dump de prompt complet)
- [x] Aucun appel IA depuis le frontend
- [x] S√©lection intelligente de mod√®le
- [ ] Tests automatis√©s pour chaque fonction IA
- [ ] Monitoring du temps de r√©ponse Ollama
- [ ] Documentation utilisateur final

---

## üéì Apprentissage

### Concepts mis en pratique

- **Architecture modulaire** : S√©paration claire des responsabilit√©s
- **Gestion d'erreur** : Fallbacks gracieux
- **Performance** : Timeout, choix de mod√®les l√©gers
- **S√©curit√©** : Validation, pas d'appels directs depuis frontend
- **Extensibilit√©** : Ajout facile de nouveaux mod√®les

### Points d'am√©lioration possibles

1. **Tests unitaires** pour les fonctions IA
2. **Mise en cache** des questions g√©n√©r√©es
3. **M√©triques** de performance et qualit√©
4. **A/B testing** de diff√©rents prompts

---

**Impl√©mentation termin√©e ! üéâ**

Le syst√®me est maintenant pr√™t √† utiliser une vraie IA locale pour g√©n√©rer des questions, √©valuer les r√©ponses et fournir des indices intelligents.
